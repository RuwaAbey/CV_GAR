{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMuBs6hZbef5JzMqFDIZhuF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuwaAbey/Finaly_Year_Project_G09/blob/main/Zoom_transformer_in_NTU_RGB_action_class_58.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "ow3PICbBRkka"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mmcv==1.5.0\n",
        "!pip install fvcore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLq5kRo2iFiK",
        "outputId": "760689cd-a4b5-45b1-c8b1-c66c0bb48cff"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mmcv==1.5.0 in /usr/local/lib/python3.11/dist-packages (1.5.0)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.11/dist-packages (from mmcv==1.5.0) (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mmcv==1.5.0) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mmcv==1.5.0) (24.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from mmcv==1.5.0) (11.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from mmcv==1.5.0) (6.0.2)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.11/dist-packages (from mmcv==1.5.0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.11/dist-packages (from yapf->mmcv==1.5.0) (4.3.6)\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.11/dist-packages (0.1.5.post20221221)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore) (1.26.4)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore) (11.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: iopath>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.1.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mmcv\n",
        "print(mmcv.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WfPYx72iIZy",
        "outputId": "94205ad9-f504-474a-fe2b-2e3e3b309aa6"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAa03GR8iK29",
        "outputId": "55562ca8-2f31-441d-c720-3d2757e88cf5"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/G_09"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVtGR6WiiNbo",
        "outputId": "02863315-e27f-4f26-9e73-713203d7a254"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/G_09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kennymckormick/pyskl.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2PlyXJEiT7s",
        "outputId": "d7739ef0-cb58-41a3-864f-1ebec9f3576b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pyskl' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/G_09/pyskl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk6QxOYAiVlb",
        "outputId": "76e4fd37-3e81-4e7f-abbb-2d01c616e75b"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/G_09/pyskl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyskl\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "1QqmkoY-iXoX"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "%cd /content/drive/My\\ Drive/G_09\n",
        "\n",
        "# Load the pickle file\n",
        "with open('ntu60_hrnet.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Check the top-level keys\n",
        "print(data.keys())  # Should print: ['split', 'annotations']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-dxKhOuiZMj",
        "outputId": "34036a70-7d5a-451d-e847-611f109127fe"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/G_09\n",
            "dict_keys(['split', 'annotations'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keypoint_pipeline = [\n",
        "    dict(type='PoseDecode'),\n",
        "    dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),\n",
        "    dict(type='Resize', scale=(-1, 64)),\n",
        "    dict(type='CenterCrop', crop_size=64),\n",
        "    dict(type='GeneratePoseTarget', with_kp=True, with_limb=False)\n",
        "]\n",
        "\n",
        "limb_pipeline = [\n",
        "    dict(type='PoseDecode'),\n",
        "    dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),\n",
        "    dict(type='Resize', scale=(-1, 64)),\n",
        "    dict(type='CenterCrop', crop_size=64),\n",
        "    dict(type='GeneratePoseTarget', with_kp=False, with_limb=True)\n",
        "]\n",
        "\n",
        "from pyskl.datasets.pipelines import Compose\n",
        "def get_pseudo_heatmap(anno, flag='keypoint'):\n",
        "    assert flag in ['keypoint', 'limb']\n",
        "    pipeline = Compose(keypoint_pipeline if flag == 'keypoint' else limb_pipeline)\n",
        "    return pipeline(anno)['imgs']\n",
        "\n",
        "def vis_heatmaps(heatmaps, channel=-1, ratio=8):\n",
        "    # if channel is -1, draw all keypoints / limbs on the same map\n",
        "    import matplotlib.cm as cm\n",
        "    heatmaps = [x.transpose(1, 2, 0) for x in heatmaps]\n",
        "    h, w, _ = heatmaps[0].shape\n",
        "    newh, neww = int(h * ratio), int(w * ratio)\n",
        "\n",
        "    if channel == -1:\n",
        "        heatmaps = [np.max(x, axis=-1) for x in heatmaps]\n",
        "    cmap = cm.viridis\n",
        "    heatmaps = [(cmap(x)[..., :3] * 255).astype(np.uint8) for x in heatmaps]\n",
        "    heatmaps = [cv2.resize(x, (neww, newh)) for x in heatmaps]\n",
        "    return heatmaps"
      ],
      "metadata": {
        "id": "J8UoS23OibBX"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the annotations field\n",
        "annotations = data['annotations']\n",
        "\n",
        "for annotation in annotations:\n",
        "    if annotation['frame_dir'] == 'S001C001P001R001A059':\n",
        "        action_58_sample = annotation\n",
        "        print(annotation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG5VMGTnie5y",
        "outputId": "0dff9791-0ef2-4324-d63f-f9652981b4ba"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'frame_dir': 'S001C001P001R001A059', 'label': 58, 'img_shape': (1080, 1920), 'original_shape': (1080, 1920), 'total_frames': 99, 'keypoint': array([[[[1433. ,  321. ],\n",
            "         [1441. ,  312.5],\n",
            "         [1429. ,  312.5],\n",
            "         ...,\n",
            "         [1425. ,  588.5],\n",
            "         [1450. ,  676. ],\n",
            "         [1425. ,  668. ]],\n",
            "\n",
            "        [[1431. ,  323.2],\n",
            "         [1439. ,  315. ],\n",
            "         [1431. ,  315. ],\n",
            "         ...,\n",
            "         [1427. ,  588. ],\n",
            "         [1448. ,  676. ],\n",
            "         [1423. ,  667.5]],\n",
            "\n",
            "        [[1433. ,  321. ],\n",
            "         [1441. ,  312.5],\n",
            "         [1429. ,  312.5],\n",
            "         ...,\n",
            "         [1425. ,  588.5],\n",
            "         [1445. ,  676. ],\n",
            "         [1425. ,  668. ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 872. ,  323.5],\n",
            "         [ 867.5,  310.8],\n",
            "         [ 863.5,  310.8],\n",
            "         ...,\n",
            "         [ 855. ,  601.5],\n",
            "         [ 859. ,  678.5],\n",
            "         [ 850.5,  691.5]],\n",
            "\n",
            "        [[ 873. ,  323.2],\n",
            "         [ 868.5,  310.5],\n",
            "         [ 864.5,  314.8],\n",
            "         ...,\n",
            "         [ 856. ,  600. ],\n",
            "         [ 856. ,  676.5],\n",
            "         [ 851.5,  693.5]],\n",
            "\n",
            "        [[1005. ,  324.2],\n",
            "         [1014. ,  319.8],\n",
            "         [1009.5,  315.5],\n",
            "         ...,\n",
            "         [1045. ,  605.5],\n",
            "         [1049. ,  702. ],\n",
            "         [1058. ,  689. ]]],\n",
            "\n",
            "\n",
            "       [[[ 471.2,  325.8],\n",
            "         [ 471.2,  317. ],\n",
            "         [ 458.2,  317. ],\n",
            "         ...,\n",
            "         [ 466.8,  605. ],\n",
            "         [ 488.2,  678. ],\n",
            "         [ 471.2,  695. ]],\n",
            "\n",
            "        [[ 469.2,  327.8],\n",
            "         [ 473.5,  319.2],\n",
            "         [ 456.2,  319.2],\n",
            "         ...,\n",
            "         [ 464.8,  606. ],\n",
            "         [ 490.5,  678.5],\n",
            "         [ 473.5,  695.5]],\n",
            "\n",
            "        [[ 469.2,  327.8],\n",
            "         [ 473.5,  319.2],\n",
            "         [ 456.2,  319.2],\n",
            "         ...,\n",
            "         [ 464.8,  606. ],\n",
            "         [ 490.5,  678.5],\n",
            "         [ 473.5,  695.5]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1009. ,  322.5],\n",
            "         [1013.5,  318.2],\n",
            "         [1009. ,  318.2],\n",
            "         ...,\n",
            "         [1045. ,  606.5],\n",
            "         [1049. ,  699.5],\n",
            "         [1058. ,  690.5]],\n",
            "\n",
            "        [[1007. ,  326.8],\n",
            "         [1011.5,  317.8],\n",
            "         [1007. ,  317.8],\n",
            "         ...,\n",
            "         [1047. ,  603.5],\n",
            "         [1051. ,  700. ],\n",
            "         [1056. ,  691.5]],\n",
            "\n",
            "        [[ 874. ,  319. ],\n",
            "         [ 869.5,  310.5],\n",
            "         [ 861. ,  314.8],\n",
            "         ...,\n",
            "         [ 857. ,  600. ],\n",
            "         [ 857. ,  676.5],\n",
            "         [ 852.5,  689.5]]]], dtype=float16), 'keypoint_score': array([[[0.9204, 0.9204, 0.9136, ..., 0.879 , 0.8906, 0.9185],\n",
            "        [0.9487, 0.963 , 0.922 , ..., 0.8804, 0.9033, 0.9385],\n",
            "        [0.9175, 0.92  , 0.9204, ..., 0.8784, 0.8984, 0.923 ],\n",
            "        ...,\n",
            "        [0.9634, 0.914 , 0.9585, ..., 0.851 , 0.8496, 0.883 ],\n",
            "        [0.963 , 0.919 , 0.9497, ..., 0.851 , 0.866 , 0.8794],\n",
            "        [0.964 , 0.934 , 0.925 , ..., 0.8457, 0.8594, 0.895 ]],\n",
            "\n",
            "       [[0.9487, 0.9326, 0.929 , ..., 0.907 , 0.9106, 0.885 ],\n",
            "        [0.959 , 0.9307, 0.978 , ..., 0.915 , 0.898 , 0.9043],\n",
            "        [0.9536, 0.9336, 0.9775, ..., 0.916 , 0.9033, 0.9087],\n",
            "        ...,\n",
            "        [0.95  , 0.954 , 0.9214, ..., 0.856 , 0.847 , 0.8716],\n",
            "        [0.9536, 0.9272, 0.922 , ..., 0.836 , 0.859 , 0.8623],\n",
            "        [0.951 , 0.9355, 0.933 , ..., 0.8535, 0.8696, 0.8926]]],\n",
            "      dtype=float16)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_58_sample_keypoint = action_58_sample['keypoint']\n",
        "#action_58_sample_keypoint"
      ],
      "metadata": {
        "id": "3r0a_fx4ihMc"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_58_sample_keypoint_score = action_58_sample['keypoint_score']\n",
        "#action_58_sample_keypoint_score"
      ],
      "metadata": {
        "id": "Pt61pjC_ijiS"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "coordinates = action_58_sample_keypoint\n",
        "confidence_scores = action_58_sample_keypoint_score\n",
        "\n",
        "# Assuming these are your original arrays\n",
        "coords = coordinates  # Replace with actual data\n",
        "conf = confidence_scores       # Replace with actual data\n",
        "\n",
        "# Transpose coordinates to (M, C, T, V)\n",
        "coords = np.transpose(coords, (0, 3, 1, 2))  # Shape: (2, 2, 99, 17)\n",
        "\n",
        "# Expand confidence scores to match dimensions (M, 1, T, V)\n",
        "conf = np.expand_dims(conf, axis=1)  # Shape: (2, 1, 99, 17)\n",
        "\n",
        "# Concatenate along the second axis (C)\n",
        "combined = np.concatenate((coords, conf), axis=1)  # Shape: (2, 3, 99, 17)\n",
        "\n",
        "# Swap M and last dimension (M should be last)\n",
        "combined = np.transpose(combined, (1, 2, 3, 0))  # Shape: (3, 99, 17, 2)\n",
        "\n",
        "# Add batch dimension (N=1)\n",
        "final_array = np.expand_dims(combined, axis=0)  # Shape: (1, 3, 99, 17, 2)\n",
        "\n",
        "print(final_array.shape)  # Expected Output: (1, 3, 99, 17, 2)\n",
        "print(final_array)\n",
        "\n",
        "x = final_array\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWL_x9o_ilxm",
        "outputId": "c7b85a00-c6c0-482f-d769-0db40641a77b"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3, 99, 17, 2)\n",
            "[[[[[1.4330e+03 4.7125e+02]\n",
            "    [1.4410e+03 4.7125e+02]\n",
            "    [1.4290e+03 4.5825e+02]\n",
            "    ...\n",
            "    [1.4250e+03 4.6675e+02]\n",
            "    [1.4500e+03 4.8825e+02]\n",
            "    [1.4250e+03 4.7125e+02]]\n",
            "\n",
            "   [[1.4310e+03 4.6925e+02]\n",
            "    [1.4390e+03 4.7350e+02]\n",
            "    [1.4310e+03 4.5625e+02]\n",
            "    ...\n",
            "    [1.4270e+03 4.6475e+02]\n",
            "    [1.4480e+03 4.9050e+02]\n",
            "    [1.4230e+03 4.7350e+02]]\n",
            "\n",
            "   [[1.4330e+03 4.6925e+02]\n",
            "    [1.4410e+03 4.7350e+02]\n",
            "    [1.4290e+03 4.5625e+02]\n",
            "    ...\n",
            "    [1.4250e+03 4.6475e+02]\n",
            "    [1.4450e+03 4.9050e+02]\n",
            "    [1.4250e+03 4.7350e+02]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[8.7200e+02 1.0090e+03]\n",
            "    [8.6750e+02 1.0135e+03]\n",
            "    [8.6350e+02 1.0090e+03]\n",
            "    ...\n",
            "    [8.5500e+02 1.0450e+03]\n",
            "    [8.5900e+02 1.0490e+03]\n",
            "    [8.5050e+02 1.0580e+03]]\n",
            "\n",
            "   [[8.7300e+02 1.0070e+03]\n",
            "    [8.6850e+02 1.0115e+03]\n",
            "    [8.6450e+02 1.0070e+03]\n",
            "    ...\n",
            "    [8.5600e+02 1.0470e+03]\n",
            "    [8.5600e+02 1.0510e+03]\n",
            "    [8.5150e+02 1.0560e+03]]\n",
            "\n",
            "   [[1.0050e+03 8.7400e+02]\n",
            "    [1.0140e+03 8.6950e+02]\n",
            "    [1.0095e+03 8.6100e+02]\n",
            "    ...\n",
            "    [1.0450e+03 8.5700e+02]\n",
            "    [1.0490e+03 8.5700e+02]\n",
            "    [1.0580e+03 8.5250e+02]]]\n",
            "\n",
            "\n",
            "  [[[3.2100e+02 3.2575e+02]\n",
            "    [3.1250e+02 3.1700e+02]\n",
            "    [3.1250e+02 3.1700e+02]\n",
            "    ...\n",
            "    [5.8850e+02 6.0500e+02]\n",
            "    [6.7600e+02 6.7800e+02]\n",
            "    [6.6800e+02 6.9500e+02]]\n",
            "\n",
            "   [[3.2325e+02 3.2775e+02]\n",
            "    [3.1500e+02 3.1925e+02]\n",
            "    [3.1500e+02 3.1925e+02]\n",
            "    ...\n",
            "    [5.8800e+02 6.0600e+02]\n",
            "    [6.7600e+02 6.7850e+02]\n",
            "    [6.6750e+02 6.9550e+02]]\n",
            "\n",
            "   [[3.2100e+02 3.2775e+02]\n",
            "    [3.1250e+02 3.1925e+02]\n",
            "    [3.1250e+02 3.1925e+02]\n",
            "    ...\n",
            "    [5.8850e+02 6.0600e+02]\n",
            "    [6.7600e+02 6.7850e+02]\n",
            "    [6.6800e+02 6.9550e+02]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[3.2350e+02 3.2250e+02]\n",
            "    [3.1075e+02 3.1825e+02]\n",
            "    [3.1075e+02 3.1825e+02]\n",
            "    ...\n",
            "    [6.0150e+02 6.0650e+02]\n",
            "    [6.7850e+02 6.9950e+02]\n",
            "    [6.9150e+02 6.9050e+02]]\n",
            "\n",
            "   [[3.2325e+02 3.2675e+02]\n",
            "    [3.1050e+02 3.1775e+02]\n",
            "    [3.1475e+02 3.1775e+02]\n",
            "    ...\n",
            "    [6.0000e+02 6.0350e+02]\n",
            "    [6.7650e+02 7.0000e+02]\n",
            "    [6.9350e+02 6.9150e+02]]\n",
            "\n",
            "   [[3.2425e+02 3.1900e+02]\n",
            "    [3.1975e+02 3.1050e+02]\n",
            "    [3.1550e+02 3.1475e+02]\n",
            "    ...\n",
            "    [6.0550e+02 6.0000e+02]\n",
            "    [7.0200e+02 6.7650e+02]\n",
            "    [6.8900e+02 6.8950e+02]]]\n",
            "\n",
            "\n",
            "  [[[9.2041e-01 9.4873e-01]\n",
            "    [9.2041e-01 9.3262e-01]\n",
            "    [9.1357e-01 9.2920e-01]\n",
            "    ...\n",
            "    [8.7891e-01 9.0723e-01]\n",
            "    [8.9062e-01 9.1064e-01]\n",
            "    [9.1846e-01 8.8477e-01]]\n",
            "\n",
            "   [[9.4873e-01 9.5898e-01]\n",
            "    [9.6289e-01 9.3066e-01]\n",
            "    [9.2188e-01 9.7803e-01]\n",
            "    ...\n",
            "    [8.8037e-01 9.1504e-01]\n",
            "    [9.0332e-01 8.9795e-01]\n",
            "    [9.3848e-01 9.0430e-01]]\n",
            "\n",
            "   [[9.1748e-01 9.5361e-01]\n",
            "    [9.1992e-01 9.3359e-01]\n",
            "    [9.2041e-01 9.7754e-01]\n",
            "    ...\n",
            "    [8.7842e-01 9.1602e-01]\n",
            "    [8.9844e-01 9.0332e-01]\n",
            "    [9.2285e-01 9.0869e-01]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[9.6338e-01 9.5020e-01]\n",
            "    [9.1406e-01 9.5410e-01]\n",
            "    [9.5850e-01 9.2139e-01]\n",
            "    ...\n",
            "    [8.5107e-01 8.5596e-01]\n",
            "    [8.4961e-01 8.4717e-01]\n",
            "    [8.8281e-01 8.7158e-01]]\n",
            "\n",
            "   [[9.6289e-01 9.5361e-01]\n",
            "    [9.1895e-01 9.2725e-01]\n",
            "    [9.4971e-01 9.2188e-01]\n",
            "    ...\n",
            "    [8.5107e-01 8.3594e-01]\n",
            "    [8.6621e-01 8.5889e-01]\n",
            "    [8.7939e-01 8.6230e-01]]\n",
            "\n",
            "   [[9.6387e-01 9.5117e-01]\n",
            "    [9.3408e-01 9.3555e-01]\n",
            "    [9.2480e-01 9.3311e-01]\n",
            "    ...\n",
            "    [8.4570e-01 8.5352e-01]\n",
            "    [8.5938e-01 8.6963e-01]\n",
            "    [8.9502e-01 8.9258e-01]]]]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3, 99, 17, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(x).float() # Convert NumPy array to PyTorch tensor\n",
        "#print(x.size())  # Now this should work"
      ],
      "metadata": {
        "id": "mJuLwf4WipAi"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from einops import rearrange\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def import_class(name):\n",
        "    components = name.split('.')\n",
        "    mod = __import__(components[0])\n",
        "    for comp in components[1:]:\n",
        "        mod = getattr(mod, comp)\n",
        "    return mod\n",
        "\n",
        "\n",
        "def conv_branch_init(conv, branches):\n",
        "    weight = conv.weight\n",
        "    n = weight.size(0)\n",
        "    k1 = weight.size(1)\n",
        "    k2 = weight.size(2)\n",
        "    nn.init.normal_(weight, 0, math.sqrt(2. / (n * k1 * k2 * branches)))\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "\n",
        "def conv_init(conv):\n",
        "    nn.init.kaiming_normal_(conv.weight, mode='fan_out')\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "\n",
        "def bn_init(bn, scale):\n",
        "    nn.init.constant_(bn.weight, scale)\n",
        "    nn.init.constant_(bn.bias, 0)\n",
        "\n",
        "class unit_tcn(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=9, stride=1):\n",
        "        super(unit_tcn, self).__init__()\n",
        "\n",
        "        pad = int((kernel_size - 1) / 2)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(pad, 0),\n",
        "                                stride=(stride, 1))\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        conv_init(self.conv)\n",
        "        bn_init(self.bn, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(self.conv(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class unit_tcn_m(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, kernel_size=[1, 3, 7]):        # ks=9 initial\n",
        "        super(unit_tcn_m, self).__init__()\n",
        "\n",
        "        pad1 = int((kernel_size[0] - 1) / 2)\n",
        "        pad2 = int((kernel_size[1] - 1) / 2)\n",
        "        pad3 = int((kernel_size[2] - 1) / 2)\n",
        "\n",
        "        mid_channels = out_channels//3\n",
        "\n",
        "        self.conv11 = nn.Conv2d(in_channels, in_channels, kernel_size=(1, 1))\n",
        "        self.conv21 = nn.Conv2d(in_channels, in_channels, kernel_size=(1, 1))\n",
        "        self.conv31 = nn.Conv2d(in_channels, in_channels, kernel_size=(1, 1))\n",
        "\n",
        "        self.conv12 = nn.Conv2d(in_channels, mid_channels, kernel_size=(kernel_size[0], 1), padding=(pad1, 0),\n",
        "                                stride=(stride, 1))\n",
        "        self.conv22 = nn.Conv2d(in_channels, mid_channels, kernel_size=(kernel_size[1], 1), padding=(pad2, 0),\n",
        "                                stride=(stride, 1))\n",
        "        self.conv32 = nn.Conv2d(in_channels, mid_channels, kernel_size=(kernel_size[2], 1), padding=(pad3, 0),\n",
        "                                stride=(stride, 1))\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        conv_init(self.conv11)\n",
        "        conv_init(self.conv21)\n",
        "        conv_init(self.conv31)\n",
        "        conv_init(self.conv12)\n",
        "        conv_init(self.conv22)\n",
        "        conv_init(self.conv32)\n",
        "        bn_init(self.bn, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv12(self.conv11(x))\n",
        "        x2 = self.conv22(self.conv21(x))\n",
        "        x3 = self.conv32(self.conv31(x))\n",
        "        x = torch.cat([x1, x2, x3], dim=1)\n",
        "        x  = self.bn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class unit_gcn(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, A, coff_embedding=4, num_subset=3):\n",
        "        super(unit_gcn, self).__init__()\n",
        "        inter_channels = out_channels // coff_embedding\n",
        "        self.inter_c = inter_channels\n",
        "        self.PA = nn.Parameter(torch.from_numpy(A.astype(np.float32)))\n",
        "        nn.init.constant_(self.PA, 1e-6)\n",
        "        self.A = Variable(torch.from_numpy(A.astype(np.float32)), requires_grad=False)\n",
        "        self.num_subset = num_subset\n",
        "\n",
        "        self.conv_a = nn.ModuleList()\n",
        "        self.conv_b = nn.ModuleList()\n",
        "        self.conv_d = nn.ModuleList()\n",
        "        for i in range(self.num_subset):\n",
        "            self.conv_a.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_b.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_d.append(nn.Conv2d(in_channels, out_channels, 1))\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.down = lambda x: x\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.soft = nn.Softmax(-2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, T, V = x.size()\n",
        "        A = self.A.to(x.device) + self.PA\n",
        "\n",
        "        y = None\n",
        "        for i in range(self.num_subset):\n",
        "            A1 = self.conv_a[i](x).permute(0, 3, 1, 2).contiguous().view(N, V, self.inter_c * T)\n",
        "            #print(f\"A1 shape {i}: {A1.shape}\")\n",
        "            A2 = self.conv_b[i](x).view(N, self.inter_c * T, V)\n",
        "            #print(f\"A2 shape {i}: {A2.shape}\")\n",
        "            A1 = self.soft(torch.matmul(A1, A2) / A1.size(-1))  # N V V\n",
        "            A1 = A1 + A[i]\n",
        "            A2 = x.view(N, C * T, V)\n",
        "            z = self.conv_d[i](torch.matmul(A2, A1).view(N, C, T, V))\n",
        "            y = z + y if y is not None else z\n",
        "\n",
        "        y = self.bn(y)\n",
        "        y += self.down(x)\n",
        "       # print(f\"y shape: {y.shape}\")\n",
        "        return self.relu(y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TCN_GCN_unit(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, A, stride=1, residual=True):\n",
        "        super(TCN_GCN_unit, self).__init__()\n",
        "        self.gcn1 = unit_gcn(in_channels, out_channels, A)\n",
        "        self.tcn1 = unit_tcn_m(out_channels, out_channels, stride=stride)\n",
        "        self.relu = nn.ReLU()\n",
        "        if not residual:\n",
        "            self.residual = lambda x: 0\n",
        "\n",
        "        elif (in_channels == out_channels) and (stride == 1):\n",
        "            self.residual = lambda x: x\n",
        "\n",
        "        else:\n",
        "            self.residual = unit_tcn(in_channels, out_channels, kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tcn1(self.gcn1(x)) + self.residual(x)\n",
        "        return self.relu(x)\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class LayerNormalize(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class MLP_Block(nn.Module):\n",
        "    def __init__(self, dim, hid_dim, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.nn1 = nn.Linear(dim, hid_dim)\n",
        "        torch.nn.init.xavier_uniform_(self.nn1.weight)\n",
        "        torch.nn.init.normal_(self.nn1.bias, std = 1e-6)\n",
        "        self.af1 = nn.ReLU()\n",
        "        self.do1 = nn.Dropout(dropout)\n",
        "        self.nn2 = nn.Linear(hid_dim, dim)\n",
        "        torch.nn.init.xavier_uniform_(self.nn2.weight)\n",
        "        torch.nn.init.normal_(self.nn2.bias, std = 1e-6)\n",
        "        self.do2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.nn1(x)\n",
        "        x = self.af1(x)\n",
        "        x = self.do1(x)\n",
        "        x = self.nn2(x)\n",
        "        x = self.do2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, out_dim, heads = 3, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5  # 1/sqrt(dim)\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, dim*3, bias = True)\n",
        "        torch.nn.init.xavier_uniform_(self.to_qkv.weight)\n",
        "        torch.nn.init.zeros_(self.to_qkv.bias)\n",
        "\n",
        "        self.nn1 = nn.Linear(dim, out_dim)\n",
        "        torch.nn.init.xavier_uniform_(self.nn1.weight)\n",
        "        torch.nn.init.zeros_(self.nn1.bias)\n",
        "        self.do1 = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x) #gets q = Q = Wq matmul x1, k = Wk mm x2, v = Wv mm x3\n",
        "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv = 3, h = h) # split into multi head attentions\n",
        "\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "\n",
        "        attn = dots.softmax(dim=-1) #follow the softmax,q,d,v equation in the paper\n",
        "\n",
        "        if mask is not None:\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            dots = (dots + mask)*0.5\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v) #product of v times whatever inside softmax\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)') #concat heads into one matrix, ready for next encoder block\n",
        "        out =  self.nn1(out)\n",
        "        out = self.do1(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        if dim == mlp_dim:\n",
        "            for _ in range(depth):\n",
        "                self.layers.append(nn.ModuleList([\n",
        "                    Residual(Attention(dim, mlp_dim, heads = heads, dropout = dropout)),\n",
        "                    Residual(LayerNormalize(mlp_dim, MLP_Block(mlp_dim, mlp_dim*2, dropout = dropout)))\n",
        "                ]))\n",
        "        else:\n",
        "            for _ in range(depth):\n",
        "                self.layers.append(nn.ModuleList([\n",
        "                    Attention(dim, mlp_dim, heads = heads, dropout = dropout),\n",
        "                    Residual(LayerNormalize(mlp_dim, MLP_Block(mlp_dim, mlp_dim*2, dropout = dropout)))\n",
        "                ]))\n",
        "    def forward(self, x, mask = None):\n",
        "        for attention, mlp in self.layers:\n",
        "            x = attention(x, mask = mask) # go to attention\n",
        "            x = mlp(x) #go to MLP_Block\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class TCN_STRANSF_unit(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, heads=3, stride=1, residual=True, dropout=0.1, mask=None, mask_grad=True):\n",
        "        super(TCN_STRANSF_unit, self).__init__()\n",
        "        self.transf1 = Transformer(dim=in_channels, depth=1, heads=heads, mlp_dim=in_channels, dropout=dropout)\n",
        "        self.tcn1 = unit_tcn_m(in_channels, out_channels, stride=stride)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.out_channels = out_channels\n",
        "        if not residual:\n",
        "            self.residual = lambda x: 0\n",
        "\n",
        "        elif (in_channels == out_channels) and (stride == 1):\n",
        "            self.residual = lambda x: x\n",
        "\n",
        "        else:\n",
        "            self.residual = unit_tcn(in_channels, out_channels, kernel_size=1, stride=stride)\n",
        "\n",
        "        if mask != None:\n",
        "            self.mask = nn.Parameter(mask, requires_grad=mask_grad)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, C, T, V= x.size()\n",
        "        tx = x.permute(0, 2, 3, 1).contiguous().view(B * T, V, C)\n",
        "        if mask==None:\n",
        "            tx = self.transf1(tx, self.mask)\n",
        "        else:\n",
        "            tx = self.transf1(tx, mask)\n",
        "        tx = tx.view(B, T, V, C).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        x = self.tcn1(tx) + self.residual(x)\n",
        "        return self.relu(x)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VqVZAGo_jgOS"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_class=15\n",
        "in_channels=3\n",
        "num_person=2\n",
        "num_point=17\n",
        "num_head=6\n",
        "data_bn = nn.BatchNorm1d(num_person * in_channels * num_point)\n",
        "bn_init(data_bn, 1)\n",
        "heads = num_head"
      ],
      "metadata": {
        "id": "g3Cogl9yi4Uz"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N, C, T, V, M = x.size()\n",
        "x = x.permute(0, 4, 3, 1, 2).contiguous().view(N, M * V * C, T)\n",
        "x = data_bn(x)\n",
        "x = x.view(N, M, V, C, T).permute(0, 1, 3, 4, 2).contiguous().view(N * M, C, T, V)"
      ],
      "metadata": {
        "id": "23EhR8kGjnKG"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_node = 17 #coco keypoints\n",
        "self_link = [(i, i) for i in range(num_node)]\n",
        "inward = [\n",
        "    (10, 8), (8, 6), (9, 7), (7, 5),  # Arms: Wrist → Elbow → Shoulder\n",
        "    (15, 13), (13, 11), (16, 14), (14, 12),  # Legs: Ankle → Knee → Hip\n",
        "    (11, 5), (12, 6), (11, 12), (5, 6),  # Torso: Hips ↔ Shoulders\n",
        "    (5, 0), (6, 0), (1, 0), (2, 0), (3, 1), (4, 2)  # Face: Nose → Eyes → Ears\n",
        "]\n",
        "outward = [(j, i) for (i, j) in inward]  # Reverse edges\n",
        "A = np.zeros((num_node, num_node))\n",
        "def edge2mat(link, num_node):\n",
        "    \"\"\" Convert edge list to adjacency matrix \"\"\"\n",
        "    A = np.zeros((num_node, num_node))  # Initialize a zero matrix\n",
        "    for i, j in link:\n",
        "        A[j, i] = 1  # Assign 1 to the adjacency matrix\n",
        "    return A\n",
        "\n",
        "#Compute Self-Link Adjacency Matrix\n",
        "I = edge2mat(self_link, num_node)\n",
        "# Compute Inward Adjacency Matrix (In)\n",
        "In = edge2mat(inward, num_node)\n",
        "# Compute Outward Adjacency Matrix (Out)\n",
        "Out = edge2mat(outward, num_node)\n",
        "A = np.stack((I, In, Out))\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5t29d1hmV4h",
        "outputId": "ca8ec8dc-15cc-45a6-e65e-c0a0e6cbf22b"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         1.]],\n",
              "\n",
              "       [[0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "         0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "         0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from einops import rearrange\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def import_class(name):\n",
        "    components = name.split('.')\n",
        "    mod = __import__(components[0])\n",
        "    for comp in components[1:]:\n",
        "        mod = getattr(mod, comp)\n",
        "    return mod\n",
        "\n",
        "\n",
        "def conv_branch_init(conv, branches):\n",
        "    weight = conv.weight\n",
        "    n = weight.size(0)\n",
        "    k1 = weight.size(1)\n",
        "    k2 = weight.size(2)\n",
        "    nn.init.normal_(weight, 0, math.sqrt(2. / (n * k1 * k2 * branches)))\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "\n",
        "def conv_init(conv):\n",
        "    nn.init.kaiming_normal_(conv.weight, mode='fan_out')\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "\n",
        "def bn_init(bn, scale):\n",
        "    nn.init.constant_(bn.weight, scale)\n",
        "    nn.init.constant_(bn.bias, 0)\n",
        "\n",
        "class unit_tcn(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=9, stride=1):\n",
        "        super(unit_tcn, self).__init__()\n",
        "\n",
        "        pad = int((kernel_size - 1) / 2)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(pad, 0),\n",
        "                                stride=(stride, 1))\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        conv_init(self.conv)\n",
        "        bn_init(self.bn, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(self.conv(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class unit_tcn_m(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, kernel_size=[1, 3, 7]):        # ks=9 initial\n",
        "        super(unit_tcn_m, self).__init__()\n",
        "\n",
        "        pad1 = int((kernel_size[0] - 1) / 2)\n",
        "        pad2 = int((kernel_size[1] - 1) / 2)\n",
        "        pad3 = int((kernel_size[2] - 1) / 2)\n",
        "\n",
        "        mid_channels = out_channels//3\n",
        "\n",
        "        self.conv11 = nn.Conv2d(in_channels, in_channels, kernel_size=(1, 1))\n",
        "        self.conv21 = nn.Conv2d(in_channels, in_channels, kernel_size=(1, 1))\n",
        "        self.conv31 = nn.Conv2d(in_channels, in_channels, kernel_size=(1, 1))\n",
        "\n",
        "        self.conv12 = nn.Conv2d(in_channels, mid_channels, kernel_size=(kernel_size[0], 1), padding=(pad1, 0),\n",
        "                                stride=(stride, 1))\n",
        "        self.conv22 = nn.Conv2d(in_channels, mid_channels, kernel_size=(kernel_size[1], 1), padding=(pad2, 0),\n",
        "                                stride=(stride, 1))\n",
        "        self.conv32 = nn.Conv2d(in_channels, mid_channels, kernel_size=(kernel_size[2], 1), padding=(pad3, 0),\n",
        "                                stride=(stride, 1))\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        conv_init(self.conv11)\n",
        "        conv_init(self.conv21)\n",
        "        conv_init(self.conv31)\n",
        "        conv_init(self.conv12)\n",
        "        conv_init(self.conv22)\n",
        "        conv_init(self.conv32)\n",
        "        bn_init(self.bn, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv12(self.conv11(x))\n",
        "        x2 = self.conv22(self.conv21(x))\n",
        "        x3 = self.conv32(self.conv31(x))\n",
        "        x = torch.cat([x1, x2, x3], dim=1)\n",
        "        x  = self.bn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class unit_gcn(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, A, coff_embedding=4, num_subset=3):\n",
        "        super(unit_gcn, self).__init__()\n",
        "        inter_channels = out_channels // coff_embedding\n",
        "        self.inter_c = inter_channels\n",
        "        self.PA = nn.Parameter(torch.from_numpy(A.astype(np.float32)))\n",
        "        nn.init.constant_(self.PA, 1e-6)\n",
        "        self.A = Variable(torch.from_numpy(A.astype(np.float32)), requires_grad=False)\n",
        "        self.num_subset = num_subset\n",
        "\n",
        "        self.conv_a = nn.ModuleList()\n",
        "        self.conv_b = nn.ModuleList()\n",
        "        self.conv_d = nn.ModuleList()\n",
        "        for i in range(self.num_subset):\n",
        "            self.conv_a.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_b.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_d.append(nn.Conv2d(in_channels, out_channels, 1))\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.down = lambda x: x\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.soft = nn.Softmax(-2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, T, V = x.size()\n",
        "        A = self.A.to(x.device) + self.PA\n",
        "\n",
        "        y = None\n",
        "        for i in range(self.num_subset):\n",
        "            A1 = self.conv_a[i](x).permute(0, 3, 1, 2).contiguous().view(N, V, self.inter_c * T)\n",
        "            #print(f\"A1 shape {i}: {A1.shape}\")\n",
        "            A2 = self.conv_b[i](x).view(N, self.inter_c * T, V)\n",
        "            #print(f\"A2 shape {i}: {A2.shape}\")\n",
        "            A1 = self.soft(torch.matmul(A1, A2) / A1.size(-1))  # N V V\n",
        "            A1 = A1 + A[i]\n",
        "            A2 = x.view(N, C * T, V)\n",
        "            z = self.conv_d[i](torch.matmul(A2, A1).view(N, C, T, V))\n",
        "            y = z + y if y is not None else z\n",
        "\n",
        "        y = self.bn(y)\n",
        "        y += self.down(x)\n",
        "       # print(f\"y shape: {y.shape}\")\n",
        "        return self.relu(y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TCN_GCN_unit(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, A, stride=1, residual=True):\n",
        "        super(TCN_GCN_unit, self).__init__()\n",
        "        self.gcn1 = unit_gcn(in_channels, out_channels, A)\n",
        "        self.tcn1 = unit_tcn_m(out_channels, out_channels, stride=stride)\n",
        "        self.relu = nn.ReLU()\n",
        "        if not residual:\n",
        "            self.residual = lambda x: 0\n",
        "\n",
        "        elif (in_channels == out_channels) and (stride == 1):\n",
        "            self.residual = lambda x: x\n",
        "\n",
        "        else:\n",
        "            self.residual = unit_tcn(in_channels, out_channels, kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tcn1(self.gcn1(x)) + self.residual(x)\n",
        "        return self.relu(x)\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class LayerNormalize(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class MLP_Block(nn.Module):\n",
        "    def __init__(self, dim, hid_dim, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.nn1 = nn.Linear(dim, hid_dim)\n",
        "        torch.nn.init.xavier_uniform_(self.nn1.weight)\n",
        "        torch.nn.init.normal_(self.nn1.bias, std = 1e-6)\n",
        "        self.af1 = nn.ReLU()\n",
        "        self.do1 = nn.Dropout(dropout)\n",
        "        self.nn2 = nn.Linear(hid_dim, dim)\n",
        "        torch.nn.init.xavier_uniform_(self.nn2.weight)\n",
        "        torch.nn.init.normal_(self.nn2.bias, std = 1e-6)\n",
        "        self.do2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.nn1(x)\n",
        "        x = self.af1(x)\n",
        "        x = self.do1(x)\n",
        "        x = self.nn2(x)\n",
        "        x = self.do2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, out_dim, heads = 3, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5  # 1/sqrt(dim)\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, dim*3, bias = True)\n",
        "        torch.nn.init.xavier_uniform_(self.to_qkv.weight)\n",
        "        torch.nn.init.zeros_(self.to_qkv.bias)\n",
        "\n",
        "        self.nn1 = nn.Linear(dim, out_dim)\n",
        "        torch.nn.init.xavier_uniform_(self.nn1.weight)\n",
        "        torch.nn.init.zeros_(self.nn1.bias)\n",
        "        self.do1 = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x) #gets q = Q = Wq matmul x1, k = Wk mm x2, v = Wv mm x3\n",
        "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv = 3, h = h) # split into multi head attentions\n",
        "\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "\n",
        "        attn = dots.softmax(dim=-1) #follow the softmax,q,d,v equation in the paper\n",
        "\n",
        "        if mask is not None:\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            dots = (dots + mask)*0.5\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v) #product of v times whatever inside softmax\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)') #concat heads into one matrix, ready for next encoder block\n",
        "        out =  self.nn1(out)\n",
        "        out = self.do1(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        if dim == mlp_dim:\n",
        "            for _ in range(depth):\n",
        "                self.layers.append(nn.ModuleList([\n",
        "                    Residual(Attention(dim, mlp_dim, heads = heads, dropout = dropout)),\n",
        "                    Residual(LayerNormalize(mlp_dim, MLP_Block(mlp_dim, mlp_dim*2, dropout = dropout)))\n",
        "                ]))\n",
        "        else:\n",
        "            for _ in range(depth):\n",
        "                self.layers.append(nn.ModuleList([\n",
        "                    Attention(dim, mlp_dim, heads = heads, dropout = dropout),\n",
        "                    Residual(LayerNormalize(mlp_dim, MLP_Block(mlp_dim, mlp_dim*2, dropout = dropout)))\n",
        "                ]))\n",
        "    def forward(self, x, mask = None):\n",
        "        for attention, mlp in self.layers:\n",
        "            x = attention(x, mask = mask) # go to attention\n",
        "            x = mlp(x) #go to MLP_Block\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class TCN_STRANSF_unit(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, heads=3, stride=1, residual=True, dropout=0.1, mask=None, mask_grad=True):\n",
        "        super(TCN_STRANSF_unit, self).__init__()\n",
        "        self.transf1 = Transformer(dim=in_channels, depth=1, heads=heads, mlp_dim=in_channels, dropout=dropout)\n",
        "        self.tcn1 = unit_tcn_m(in_channels, out_channels, stride=stride)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.out_channels = out_channels\n",
        "        if not residual:\n",
        "            self.residual = lambda x: 0\n",
        "\n",
        "        elif (in_channels == out_channels) and (stride == 1):\n",
        "            self.residual = lambda x: x\n",
        "\n",
        "        else:\n",
        "            self.residual = unit_tcn(in_channels, out_channels, kernel_size=1, stride=stride)\n",
        "\n",
        "        if mask != None:\n",
        "            self.mask = nn.Parameter(mask, requires_grad=mask_grad)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, C, T, V= x.size()\n",
        "        tx = x.permute(0, 2, 3, 1).contiguous().view(B * T, V, C)\n",
        "        if mask==None:\n",
        "            tx = self.transf1(tx, self.mask)\n",
        "        else:\n",
        "            tx = self.transf1(tx, mask)\n",
        "        tx = tx.view(B, T, V, C).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        x = self.tcn1(tx) + self.residual(x)\n",
        "        return self.relu(x)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w8zfWei9iyS_"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = TCN_GCN_unit(3, 48, A, residual=False)\n",
        "output_layer_1 = l1(x)"
      ],
      "metadata": {
        "id": "wYRTTfNOlm5v"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyKzM8bxm9HD",
        "outputId": "9aedb96f-2136-400d-909c-fe12b0f9d9c1"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 99, 17])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer_1.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L76KBJ2ym_1b",
        "outputId": "1320eb7c-8679-40bd-db53-de8e28eaf59f"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 48, 99, 17])"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "A_0 = torch.from_numpy(A[0].astype(np.float32))\n",
        "l1 = TCN_GCN_unit(3, 48, A, residual=False)\n",
        "l2 = TCN_STRANSF_unit(48, 48, heads=num_head, mask=A_0, mask_grad=False)\n",
        "l3 = TCN_STRANSF_unit(48, 48, heads=num_head, mask=A_0, mask_grad=False)\n",
        "l4 = TCN_STRANSF_unit(48, 96, heads=num_head, stride=2, mask=A_0, mask_grad=True)\n",
        "l5 = TCN_STRANSF_unit(96, 96, heads=num_head, mask=A_0, mask_grad=True)\n",
        "l6 = TCN_STRANSF_unit(96, 192, heads=num_head, stride=2, mask=A_0, mask_grad=True)\n",
        "l7 = TCN_STRANSF_unit(192, 192, heads=num_head, mask=A_0, mask_grad=True)"
      ],
      "metadata": {
        "id": "RpGctSAro8K_"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer_1 = l1(x)\n",
        "output_layer_2 = l2(output_layer_1)\n",
        "output_layer_3 = l3(output_layer_2)\n",
        "output_layer_4 = l4(output_layer_3)\n",
        "output_layer_5 = l5(output_layer_4)\n",
        "output_layer_6 = l6(output_layer_5)\n",
        "output_layer_7 = l7(output_layer_6)"
      ],
      "metadata": {
        "id": "OpurAPcAphuz"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer_7.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3EM2NJWqT00",
        "outputId": "96a7cea8-ee17-4a0f-9132-14656542722c"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 192, 25, 17])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B, C_, T_, V_ = output_layer_7.size()\n",
        "output_ZiT = output_layer_7.view(N, M, C_, T_, V_).mean(4)\n",
        "output_ZiT = output_ZiT.permute(0, 2, 3, 1).contiguous()"
      ],
      "metadata": {
        "id": "WKqVypdUqfuE"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_ZiT.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D8knDNNqjkv",
        "outputId": "a4b89404-4f27-4bc5-9063-87dfa0b47224"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 192, 25, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Example output tensor from ZiT (Replace with actual tensor)\n",
        "#ZiT_output = torch.rand(1, 192, 25, 2)  # Shape: [1, 192, 25, 2]\n",
        "\n",
        "\n",
        "# Remove batch dimension\n",
        "feature_map = output_ZiT[0]  # Shape: [192, 25, 2]\n",
        "\n",
        "# Compute attention scores by averaging over channels\n",
        "attention_map = feature_map.mean(dim=0)  # Shape: [25, 2] (Time frames × Persons)\n",
        "\n",
        "# Convert to NumPy for visualization\n",
        "attention_map_np = attention_map.detach().cpu().numpy()\n",
        "\n",
        "# Plot the heatmap for each person separately\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "for i in range(2):  # Loop over persons\n",
        "    axes[i].imshow(attention_map_np[:, i].reshape(25, 1), cmap='viridis', aspect='auto')\n",
        "    axes[i].set_title(f\"Person {i+1}\")\n",
        "    axes[i].set_xlabel(\"Time\")\n",
        "    axes[i].set_ylabel(\"Channels\")\n",
        "    axes[i].invert_yaxis()  # To match the paper's view\n",
        "\n",
        "plt.colorbar(axes[0].imshow(attention_map_np[:, 0].reshape(25, 1), cmap='viridis', aspect='auto'),\n",
        "             ax=axes, orientation='vertical', fraction=0.05)\n",
        "plt.suptitle(\"Attention Map from ZiT\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "ZfL9sf3rr89K",
        "outputId": "b8745ae6-d160-4166-97de-8ec78cb426df"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}